apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: infra-cluster-master
  region: eu-north-1
  tags:
    environment: test
    owner: platform
nodeGroups:
- desiredCapacity: 3
  iam:
    withAddonPolicies:
      albIngress: true
      autoScaler: true
      certManager: true
      ebs: true
      externalDNS: true
      imageBuilder: true
  instanceType: m5.large
  kubeletExtraConfig:
    evictionHard:
      memory.available: 200Mi
      nodefs.available: 10%
    featureGates:
      DynamicKubeletConfig: true
      RotateKubeletServerCertificate: true
    kubeReserved:
      cpu: 300m
      ephemeral-storage: 1Gi
      memory: 1Gi
    kubeReservedCgroup: /kube-reserved
    systemReserved:
      cpu: 300m
      ephemeral-storage: 1Gi
      memory: 300Mi
  labels:
    node-role.kubernetes.io/infra: "true"
    node-type.ohoy.io: infra
    role: infra
  maxSize: 5
  minSize: 2
  name: infra-cluster-master-infra
  preBootstrapCommands:
  - |
    cat <<EOF > /etc/sysctl.d/10-disable-ipv6.conf
    # disable ipv6 config
    net.ipv6.conf.all.disable_ipv6 = 1
    net.ipv6.conf.default.disable_ipv6 = 1
    net.ipv6.conf.lo.disable_ipv6 = 1
    EOF
  - |
    cat <<EOF > /etc/sysctl.d/99-kube-net.conf
    # Have a larger connection range available
    net.ipv4.ip_local_port_range=1024 65000

    # Reuse closed sockets faster
    net.ipv4.tcp_tw_reuse=1
    net.ipv4.tcp_fin_timeout=15

    # The maximum number of "backlogged sockets".  Default is 128.
    net.core.somaxconn=4096
    net.core.netdev_max_backlog=4096

    # 16MB per socket - which sounds like a lot,
    # but will virtually never consume that much.
    net.core.rmem_max=16777216
    net.core.wmem_max=16777216

    # Various network tunables
    net.ipv4.tcp_max_syn_backlog=20480
    net.ipv4.tcp_max_tw_buckets=400000
    net.ipv4.tcp_no_metrics_save=1
    net.ipv4.tcp_rmem=4096 87380 16777216
    net.ipv4.tcp_syn_retries=2
    net.ipv4.tcp_synack_retries=2
    net.ipv4.tcp_wmem=4096 65536 16777216
    #vm.min_free_kbytes=65536

    # Connection tracking to prevent dropped connections (usually issue on LBs)
    net.netfilter.nf_conntrack_max=262144
    net.ipv4.netfilter.ip_conntrack_generic_timeout=120
    net.netfilter.nf_conntrack_tcp_timeout_established=86400

    # ARP cache settings for a highly loaded docker swarm
    net.ipv4.neigh.default.gc_thresh1=8096
    net.ipv4.neigh.default.gc_thresh2=12288
    net.ipv4.neigh.default.gc_thresh3=16384
    EOF
  - systemctl restart systemd-sysctl.service
  privateNetworking: true
  ssh:
    publicKeyName: platform
  volumeEncrypted: true
  volumeSize: 100
  volumeType: gp2
- desiredCapacity: 3
  iam:
    withAddonPolicies:
      albIngress: true
      ebs: true
  instanceType: m5.large
  kubeletExtraConfig:
    evictionHard:
      memory.available: 200Mi
      nodefs.available: 10%
    featureGates:
      DynamicKubeletConfig: true
      RotateKubeletServerCertificate: true
    kubeReserved:
      cpu: 300m
      ephemeral-storage: 1Gi
      memory: 1Gi
    kubeReservedCgroup: /kube-reserved
    systemReserved:
      cpu: 300m
      ephemeral-storage: 1Gi
      memory: 300Mi
  labels:
    node-role.kubernetes.io/worker: "true"
    node-type.ohoy.io: app
    role: workers
  maxSize: 5
  minSize: 2
  name: infra-cluster-master-workers
  preBootstrapCommands:
  - |
    cat <<EOF > /etc/sysctl.d/10-disable-ipv6.conf
    # disable ipv6 config
    net.ipv6.conf.all.disable_ipv6 = 1
    net.ipv6.conf.default.disable_ipv6 = 1
    net.ipv6.conf.lo.disable_ipv6 = 1
    EOF
  - |
    cat <<EOF > /etc/sysctl.d/99-kube-net.conf
    # Have a larger connection range available
    net.ipv4.ip_local_port_range=1024 65000

    # Reuse closed sockets faster
    net.ipv4.tcp_tw_reuse=1
    net.ipv4.tcp_fin_timeout=15

    # The maximum number of "backlogged sockets".  Default is 128.
    net.core.somaxconn=4096
    net.core.netdev_max_backlog=4096

    # 16MB per socket - which sounds like a lot,
    # but will virtually never consume that much.
    net.core.rmem_max=16777216
    net.core.wmem_max=16777216

    # Various network tunables
    net.ipv4.tcp_max_syn_backlog=20480
    net.ipv4.tcp_max_tw_buckets=400000
    net.ipv4.tcp_no_metrics_save=1
    net.ipv4.tcp_rmem=4096 87380 16777216
    net.ipv4.tcp_syn_retries=2
    net.ipv4.tcp_synack_retries=2
    net.ipv4.tcp_wmem=4096 65536 16777216
    #vm.min_free_kbytes=65536

    # Connection tracking to prevent dropped connections (usually issue on LBs)
    net.netfilter.nf_conntrack_max=262144
    net.ipv4.netfilter.ip_conntrack_generic_timeout=120
    net.netfilter.nf_conntrack_tcp_timeout_established=86400

    # ARP cache settings for a highly loaded docker swarm
    net.ipv4.neigh.default.gc_thresh1=8096
    net.ipv4.neigh.default.gc_thresh2=12288
    net.ipv4.neigh.default.gc_thresh3=16384
    EOF
  - systemctl restart systemd-sysctl.service
  privateNetworking: true
  ssh:
    publicKeyName: platform
  volumeEncrypted: true
  volumeSize: 100
  volumeType: gp2
- desiredCapacity: 3
  iam:
    withAddonPolicies:
      albIngress: true
      ebs: true
  instancesDistribution:
    instanceTypes:
    - m5.large
    - m5.xlarge
  kubeletExtraConfig:
    evictionHard:
      memory.available: 200Mi
      nodefs.available: 10%
    featureGates:
      DynamicKubeletConfig: true
      RotateKubeletServerCertificate: true
    kubeReserved:
      cpu: 300m
      ephemeral-storage: 1Gi
      memory: 1Gi
    kubeReservedCgroup: /kube-reserved
    systemReserved:
      cpu: 300m
      ephemeral-storage: 1Gi
      memory: 300Mi
  labels:
    node-role.kubernetes.io/worker: "true"
    node-type.ohoy.io: spot
    role: workers
  maxSize: 5
  minSize: 2
  name: infra-cluster-master-spot
  preBootstrapCommands:
  - |
    cat <<EOF > /etc/sysctl.d/10-disable-ipv6.conf
    # disable ipv6 config
    net.ipv6.conf.all.disable_ipv6 = 1
    net.ipv6.conf.default.disable_ipv6 = 1
    net.ipv6.conf.lo.disable_ipv6 = 1
    EOF
  - |
    cat <<EOF > /etc/sysctl.d/99-kube-net.conf
    # Have a larger connection range available
    net.ipv4.ip_local_port_range=1024 65000

    # Reuse closed sockets faster
    net.ipv4.tcp_tw_reuse=1
    net.ipv4.tcp_fin_timeout=15

    # The maximum number of "backlogged sockets".  Default is 128.
    net.core.somaxconn=4096
    net.core.netdev_max_backlog=4096

    # 16MB per socket - which sounds like a lot,
    # but will virtually never consume that much.
    net.core.rmem_max=16777216
    net.core.wmem_max=16777216

    # Various network tunables
    net.ipv4.tcp_max_syn_backlog=20480
    net.ipv4.tcp_max_tw_buckets=400000
    net.ipv4.tcp_no_metrics_save=1
    net.ipv4.tcp_rmem=4096 87380 16777216
    net.ipv4.tcp_syn_retries=2
    net.ipv4.tcp_synack_retries=2
    net.ipv4.tcp_wmem=4096 65536 16777216
    #vm.min_free_kbytes=65536

    # Connection tracking to prevent dropped connections (usually issue on LBs)
    net.netfilter.nf_conntrack_max=262144
    net.ipv4.netfilter.ip_conntrack_generic_timeout=120
    net.netfilter.nf_conntrack_tcp_timeout_established=86400

    # ARP cache settings for a highly loaded docker swarm
    net.ipv4.neigh.default.gc_thresh1=8096
    net.ipv4.neigh.default.gc_thresh2=12288
    net.ipv4.neigh.default.gc_thresh3=16384
    EOF
  - systemctl restart systemd-sysctl.service
  privateNetworking: true
  ssh:
    publicKeyName: platform
  taints:
    node-type: spot:NoSchedule
  volumeEncrypted: true
  volumeSize: 100
  volumeType: gp2
vpc:
  cidr: 10.8.0.0/16
  id: vpc-041fd1a6f86db3907
  subnets:
    private:
      eu-north-1a:
        cidr: 10.8.0.0/21
        id: subnet-00544d341582321e9
      eu-north-1b:
        cidr: 10.8.8.0/21
        id: subnet-0b1c35e2317dd4acd
      eu-north-1c:
        cidr: 10.8.16.0/21
        id: subnet-059c0f88e61fae7bb
    public:
      eu-north-1a:
        cidr: 10.8.24.0/23
        id: subnet-052002bc830362604
      eu-north-1b:
        cidr: 10.8.26.0/23
        id: subnet-08bc9d4c0db7e580f
      eu-north-1c:
        cidr: 10.8.28.0/23
        id: subnet-008ce331e51e85839
